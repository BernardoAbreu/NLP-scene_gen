{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:57:28.584386Z",
     "start_time": "2018-11-14T16:57:28.560715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/bernardoabreu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bernardoabreu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import sklearn\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from random import randint\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:09:08.074186Z",
     "start_time": "2018-11-14T21:09:08.050469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2811\n"
     ]
    }
   ],
   "source": [
    "filename = '../changes/10thingsihateaboutyou.txt'\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    movie = [(line[1],line[6:-2]) for line in f]\n",
    "\n",
    "movie = [t if t[0] not in ('P, E') else (t[0],t[1][1:-1]) for t in movie]\n",
    "print(len(movie))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:09:26.041887Z",
     "start_time": "2018-11-14T21:09:25.016005Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_tokens = [(t[0], nltk.tokenize.word_tokenize(t[1])) for t in movie]\n",
    "# movie_no_stopwords = []\n",
    "# for t, m_list in movie_tokens:\n",
    "#     new_movie = (t, [m for m in m_list if m not in set(stopwords.words('english'))])\n",
    "#     if new_movie[1]:\n",
    "#         movie_no_stopwords.append(new_movie)\n",
    "# regextokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "# movie_no_punct = [(t[0], regextokenizer.tokenize(t[1])) for t in movie]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:11.074102Z",
     "start_time": "2018-11-14T21:10:11.060733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('D', ['so', 'they', 'tell', 'me', '...'])\n",
      "2811\n"
     ]
    }
   ],
   "source": [
    "print(movie_tokens[50])\n",
    "print(len(movie_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T14:18:01.944184Z",
     "start_time": "2018-11-14T14:15:18.675445Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\"../data/processed/word2vec.6b.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T14:18:31.428518Z",
     "start_time": "2018-11-14T14:18:27.698248Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(w2v_model, open('../data/processed/word2vec_model_6b_300.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split movie and get list of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:23.962540Z",
     "start_time": "2018-11-14T21:10:23.950880Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_movies(movie_tokens):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for tag, seq in movie_tokens:\n",
    "        block = [word for word in seq if word in w2v_model.vocab]\n",
    "        tokens.extend(block)\n",
    "        tags.extend(([tag]*len(block)))\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:28.052373Z",
     "start_time": "2018-11-14T21:10:28.018944Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens, tags = split_movies(movie_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize into sequences of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:31.697686Z",
     "start_time": "2018-11-14T21:10:31.508849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 21736\n",
      "['high', 'school', 'day', 'welcome', 'to', 'padua', 'high', 'school', ',', 'your', 'typical', 'urban', 'suburban', 'high', 'school', 'in', 'portland', ',', 'oregon', '.', 'smarties', ',', 'skids', ',', 'preppies', ',', '.', 'loners', ',', 'lovers', ',', 'the', 'in', 'and', 'the', 'out', 'crowd', 'rub', 'sleep', 'out', 'of', 'their', 'eyes', 'and', 'head', 'for', 'the', 'main', 'building', '.', 'padua']\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_LENGTH = 50\n",
    "sequences = [list(ngram) for ngram in nltk.ngrams(tokens, SENTENCE_LENGTH + 1)]\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "print(sequences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save sequences to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:35.705655Z",
     "start_time": "2018-11-14T21:10:35.575742Z"
    }
   },
   "outputs": [],
   "source": [
    "out_filename = 'movie_sequences.txt'\n",
    "with open(out_filename, 'w') as f:\n",
    "    f.write('\\n'.join([' '.join(line) for line in sequences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sequences from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:38.451292Z",
     "start_time": "2018-11-14T21:10:37.737806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21736, 51)\n"
     ]
    }
   ],
   "source": [
    "in_filename = 'movie_sequences.txt'\n",
    "df_seq = pd.read_csv(in_filename, sep=' ', prefix='X', header=None)\n",
    "df_seq.rename(columns={'X'+ str(length - 1): 'Y'}, inplace=True)\n",
    "print(df_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:43.355169Z",
     "start_time": "2018-11-14T21:10:43.301283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X41</th>\n",
       "      <th>X42</th>\n",
       "      <th>X43</th>\n",
       "      <th>X44</th>\n",
       "      <th>X45</th>\n",
       "      <th>X46</th>\n",
       "      <th>X47</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>padua</td>\n",
       "      <td>high</td>\n",
       "      <td>school</td>\n",
       "      <td>day</td>\n",
       "      <td>welcome</td>\n",
       "      <td>to</td>\n",
       "      <td>padua</td>\n",
       "      <td>high</td>\n",
       "      <td>school</td>\n",
       "      <td>,</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>their</td>\n",
       "      <td>eyes</td>\n",
       "      <td>and</td>\n",
       "      <td>head</td>\n",
       "      <td>for</td>\n",
       "      <td>the</td>\n",
       "      <td>main</td>\n",
       "      <td>building</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>school</td>\n",
       "      <td>day</td>\n",
       "      <td>welcome</td>\n",
       "      <td>to</td>\n",
       "      <td>padua</td>\n",
       "      <td>high</td>\n",
       "      <td>school</td>\n",
       "      <td>,</td>\n",
       "      <td>your</td>\n",
       "      <td>...</td>\n",
       "      <td>their</td>\n",
       "      <td>eyes</td>\n",
       "      <td>and</td>\n",
       "      <td>head</td>\n",
       "      <td>for</td>\n",
       "      <td>the</td>\n",
       "      <td>main</td>\n",
       "      <td>building</td>\n",
       "      <td>.</td>\n",
       "      <td>padua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>school</td>\n",
       "      <td>day</td>\n",
       "      <td>welcome</td>\n",
       "      <td>to</td>\n",
       "      <td>padua</td>\n",
       "      <td>high</td>\n",
       "      <td>school</td>\n",
       "      <td>,</td>\n",
       "      <td>your</td>\n",
       "      <td>typical</td>\n",
       "      <td>...</td>\n",
       "      <td>eyes</td>\n",
       "      <td>and</td>\n",
       "      <td>head</td>\n",
       "      <td>for</td>\n",
       "      <td>the</td>\n",
       "      <td>main</td>\n",
       "      <td>building</td>\n",
       "      <td>.</td>\n",
       "      <td>padua</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day</td>\n",
       "      <td>welcome</td>\n",
       "      <td>to</td>\n",
       "      <td>padua</td>\n",
       "      <td>high</td>\n",
       "      <td>school</td>\n",
       "      <td>,</td>\n",
       "      <td>your</td>\n",
       "      <td>typical</td>\n",
       "      <td>urban</td>\n",
       "      <td>...</td>\n",
       "      <td>and</td>\n",
       "      <td>head</td>\n",
       "      <td>for</td>\n",
       "      <td>the</td>\n",
       "      <td>main</td>\n",
       "      <td>building</td>\n",
       "      <td>.</td>\n",
       "      <td>padua</td>\n",
       "      <td>high</td>\n",
       "      <td>parking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>welcome</td>\n",
       "      <td>to</td>\n",
       "      <td>padua</td>\n",
       "      <td>high</td>\n",
       "      <td>school</td>\n",
       "      <td>,</td>\n",
       "      <td>your</td>\n",
       "      <td>typical</td>\n",
       "      <td>urban</td>\n",
       "      <td>suburban</td>\n",
       "      <td>...</td>\n",
       "      <td>head</td>\n",
       "      <td>for</td>\n",
       "      <td>the</td>\n",
       "      <td>main</td>\n",
       "      <td>building</td>\n",
       "      <td>.</td>\n",
       "      <td>padua</td>\n",
       "      <td>high</td>\n",
       "      <td>parking</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X0       X1       X2       X3       X4      X5      X6       X7  \\\n",
       "0    padua     high   school      day  welcome      to   padua     high   \n",
       "1     high   school      day  welcome       to   padua    high   school   \n",
       "2   school      day  welcome       to    padua    high  school        ,   \n",
       "3      day  welcome       to    padua     high  school       ,     your   \n",
       "4  welcome       to    padua     high   school       ,    your  typical   \n",
       "\n",
       "        X8        X9   ...       X41    X42   X43   X44       X45       X46  \\\n",
       "0   school         ,   ...        of  their  eyes   and      head       for   \n",
       "1        ,      your   ...     their   eyes   and  head       for       the   \n",
       "2     your   typical   ...      eyes    and  head   for       the      main   \n",
       "3  typical     urban   ...       and   head   for   the      main  building   \n",
       "4    urban  suburban   ...      head    for   the  main  building         .   \n",
       "\n",
       "        X47       X48       X49        Y  \n",
       "0       the      main  building        .  \n",
       "1      main  building         .    padua  \n",
       "2  building         .     padua     high  \n",
       "3         .     padua      high  parking  \n",
       "4     padua      high   parking      lot  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer encode sequences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:45.146974Z",
     "start_time": "2018-11-14T21:10:45.136514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21736, 51)\n"
     ]
    }
   ],
   "source": [
    "print(df_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:46.949593Z",
     "start_time": "2018-11-14T21:10:46.944874Z"
    }
   },
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return w2v_model.vocab[word].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:51.060287Z",
     "start_time": "2018-11-14T21:10:48.900585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X41</th>\n",
       "      <th>X42</th>\n",
       "      <th>X43</th>\n",
       "      <th>X44</th>\n",
       "      <th>X45</th>\n",
       "      <th>X46</th>\n",
       "      <th>X47</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29318</td>\n",
       "      <td>152</td>\n",
       "      <td>164</td>\n",
       "      <td>122</td>\n",
       "      <td>3143</td>\n",
       "      <td>4</td>\n",
       "      <td>29318</td>\n",
       "      <td>152</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>2251</td>\n",
       "      <td>5</td>\n",
       "      <td>362</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>444</td>\n",
       "      <td>447</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152</td>\n",
       "      <td>164</td>\n",
       "      <td>122</td>\n",
       "      <td>3143</td>\n",
       "      <td>4</td>\n",
       "      <td>29318</td>\n",
       "      <td>152</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>392</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>2251</td>\n",
       "      <td>5</td>\n",
       "      <td>362</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>444</td>\n",
       "      <td>447</td>\n",
       "      <td>2</td>\n",
       "      <td>29318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>164</td>\n",
       "      <td>122</td>\n",
       "      <td>3143</td>\n",
       "      <td>4</td>\n",
       "      <td>29318</td>\n",
       "      <td>152</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>392</td>\n",
       "      <td>3682</td>\n",
       "      <td>...</td>\n",
       "      <td>2251</td>\n",
       "      <td>5</td>\n",
       "      <td>362</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>444</td>\n",
       "      <td>447</td>\n",
       "      <td>2</td>\n",
       "      <td>29318</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>3143</td>\n",
       "      <td>4</td>\n",
       "      <td>29318</td>\n",
       "      <td>152</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>392</td>\n",
       "      <td>3682</td>\n",
       "      <td>2227</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>362</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>444</td>\n",
       "      <td>447</td>\n",
       "      <td>2</td>\n",
       "      <td>29318</td>\n",
       "      <td>152</td>\n",
       "      <td>4625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3143</td>\n",
       "      <td>4</td>\n",
       "      <td>29318</td>\n",
       "      <td>152</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>392</td>\n",
       "      <td>3682</td>\n",
       "      <td>2227</td>\n",
       "      <td>5151</td>\n",
       "      <td>...</td>\n",
       "      <td>362</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>444</td>\n",
       "      <td>447</td>\n",
       "      <td>2</td>\n",
       "      <td>29318</td>\n",
       "      <td>152</td>\n",
       "      <td>4625</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X0    X1     X2     X3     X4     X5     X6    X7    X8    X9  ...    \\\n",
       "0  29318   152    164    122   3143      4  29318   152   164     1  ...     \n",
       "1    152   164    122   3143      4  29318    152   164     1   392  ...     \n",
       "2    164   122   3143      4  29318    152    164     1   392  3682  ...     \n",
       "3    122  3143      4  29318    152    164      1   392  3682  2227  ...     \n",
       "4   3143     4  29318    152    164      1    392  3682  2227  5151  ...     \n",
       "\n",
       "    X41   X42   X43  X44  X45  X46    X47    X48    X49      Y  \n",
       "0     3    44  2251    5  362   10      0    444    447      2  \n",
       "1    44  2251     5  362   10    0    444    447      2  29318  \n",
       "2  2251     5   362   10    0  444    447      2  29318    152  \n",
       "3     5   362    10    0  444  447      2  29318    152   4625  \n",
       "4   362    10     0  444  447    2  29318    152   4625    530  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seq = df_seq.applymap(word2idx)\n",
    "df_seq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:10:54.593958Z",
     "start_time": "2018-11-14T21:10:54.569804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21736, 50)\n",
      "(21736,)\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "X = df_seq.drop('Y', axis=1)\n",
    "Y = df_seq['Y']\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(len(w2v_model.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:11:06.948335Z",
     "start_time": "2018-11-14T21:11:06.941224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fdc07471ef0>"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:11:12.577402Z",
     "start_time": "2018-11-14T21:11:08.816281Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size, embedding_size = w2v_model.vectors.shape\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_size,\n",
    "        input_length=(SENTENCE_LENGTH),\n",
    "        weights=[w2v_model.vectors]\n",
    "        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add LSTM Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:11:15.458400Z",
     "start_time": "2018-11-14T21:11:13.557657Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(keras.layers.LSTM(50, return_sequences=True))\n",
    "model.add(keras.layers.LSTM(50))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:11:16.367054Z",
     "start_time": "2018-11-14T21:11:16.351609Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 50, 100)           40000000  \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 50, 50)            30200     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 400000)            20400000  \n",
      "=================================================================\n",
      "Total params: 60,452,950\n",
      "Trainable params: 60,452,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:11:19.373721Z",
     "start_time": "2018-11-14T21:11:19.276643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model\n"
     ]
    }
   ],
   "source": [
    "print('Compile model')\n",
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:55:45.516633Z",
     "start_time": "2018-11-14T21:16:13.085442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training\n",
      "Epoch 1/2\n",
      "21736/21736 [==============================] - 1356s 62ms/step - loss: 7.4649 - acc: 0.0683\n",
      "Epoch 2/2\n",
      "21736/21736 [==============================] - 1013s 47ms/step - loss: 6.1749 - acc: 0.0689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc29728fd0>"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Begin training')\n",
    "# fit model\n",
    "model.fit(X, Y, batch_size=64, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T22:10:13.382948Z",
     "start_time": "2018-11-14T22:09:56.375395Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:00:20.939018Z",
     "start_time": "2018-11-14T16:00:20.929760Z"
    }
   },
   "outputs": [],
   "source": [
    "def idx2word(idx):\n",
    "    return w2v_model.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:55:56.891963Z",
     "start_time": "2018-11-14T21:55:56.409646Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_seq(model, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "#     print(seed_text)\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = np.array([word2idx(word) for word in in_text])\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict([encoded], verbose=0)\n",
    "        possibilites\n",
    "        prob = yhat[0] / yhat[0].sum(0)\n",
    "        index = np.random.choice(len(w2v_model.vocab), p=prob)\n",
    "#         # map predicted word index to word\n",
    "        out_word = idx2word(index)\n",
    "#         # append to input\n",
    "        in_text.append(out_word)\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:55:59.585745Z",
     "start_time": "2018-11-14T21:55:59.460775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walks', 'her', 'over', 'to', 'the', 'swingset', 'and', 'plops', 'her', 'down', 'in', 'a', 'swing', ',', 'moving', 'her', 'hands', 'to', 'hang', 'onto', 'the', 'chains', '.', 'patrick', 'how', \"'s\", 'that', '?', 'she', 'sits', 'and', 'looks', 'at', 'him', 'for', 'a', 'moment', 'with', 'a', 'smile', '.', 'then', 'falls', 'over', 'backward', '.', 'patrick', 'jesus', '.', 'you']\n"
     ]
    }
   ],
   "source": [
    "seed_text = sequences[randint(0,len(sequences))][:50]\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T21:56:09.512587Z",
     "start_time": "2018-11-14T21:56:04.909723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walks her over to the swingset and plops her down in a swing , moving her hands to hang onto the chains . patrick how 's that ? she sits and looks at him for a moment with a smile . then falls over backward . patrick jesus . you sarah cameron flood we did the with is favorite . . turns and out 's him comment you think ? never listen like knew christ her as cameron n't does stop her bogey certain look she a backpedal i takes her sister lou today opportunity i let back sassy bianca\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model, 50, seed_text[:50], 50)\n",
    "print(' '.join(seed_text) + ' ' + generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
